{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1-A3f6R_Oo1"
      },
      "source": [
        "## Sentence generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIqT579V_TpF"
      },
      "source": [
        "### Importation des modèles et données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_dptkEM-Cf4",
        "outputId": "90e9899d-1836-4960-b330-2f9de5ef4243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwOLUesr-ImX"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BbDbuwJ_aeb"
      },
      "source": [
        "Changement de l'environnement de travail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQSuwvea-KDp",
        "outputId": "9bc6899c-4de1-451d-ad73-d4f6f442cfbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Sentence-VAE\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Sentence-VAE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoibN1o_nIu"
      },
      "source": [
        "### Apprentissage de notre modèle\n",
        "L'apprentissage ce fera sur toutes les classes (un choix qui ce justifie à cause de la taille de la classe la plus répresntée qui est d'environ 900 donc avec les différentes division, la base d'entrainement sera très petite).\n",
        "\n",
        "Pour faire l'apprentissage, nous jouerons sur les hyperparamètres pour essayer d'ameliorer les performance de notre modèles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFR6f0HD-WIl",
        "outputId": "7b217e8c-66e3-4f38-b9f5-5be44aa32543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentenceVAE(\n",
            "  (embedding): Embedding(65431, 300)\n",
            "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
            "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
            "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
            "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
            "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
            "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
            "  (outputs2vocab): Linear(in_features=256, out_features=65431, bias=True)\n",
            ")\n",
            "TRAIN Batch 0000/188, Loss  638.6732, NLL-Loss  638.6724, KL-Loss    0.4009, KL-Weight  0.002\n",
            "TRAIN Batch 0050/188, Loss  432.3401, NLL-Loss  432.2119, KL-Loss   58.7429, KL-Weight  0.002\n",
            "TRAIN Batch 0100/188, Loss  400.0733, NLL-Loss  399.9678, KL-Loss   42.6757, KL-Weight  0.002\n",
            "TRAIN Batch 0150/188, Loss  377.5901, NLL-Loss  377.4618, KL-Loss   45.7914, KL-Weight  0.003\n",
            "TRAIN Batch 0188/188, Loss  385.7993, NLL-Loss  385.6482, KL-Loss   49.0619, KL-Weight  0.003\n",
            "TRAIN Epoch 00/10, Mean ELBO  417.5675\n",
            "Model saved at bin/2023-Apr-10-21:05:19/E0.pytorch\n",
            "VALID Batch 0000/47, Loss  351.9118, NLL-Loss  351.7525, KL-Loss   51.5876, KL-Weight  0.003\n",
            "VALID Batch 0047/47, Loss  335.0176, NLL-Loss  334.8692, KL-Loss   48.0621, KL-Weight  0.003\n",
            "VALID Epoch 00/10, Mean ELBO  347.4707\n",
            "TRAIN Batch 0000/188, Loss  368.4040, NLL-Loss  368.2489, KL-Loss   50.2569, KL-Weight  0.003\n",
            "TRAIN Batch 0050/188, Loss  347.2708, NLL-Loss  347.0647, KL-Loss   58.9374, KL-Weight  0.003\n",
            "TRAIN Batch 0100/188, Loss  345.5709, NLL-Loss  345.3253, KL-Loss   62.0197, KL-Weight  0.004\n",
            "TRAIN Batch 0150/188, Loss  326.2882, NLL-Loss  326.0218, KL-Loss   59.3920, KL-Weight  0.004\n",
            "TRAIN Batch 0188/188, Loss  335.9057, NLL-Loss  335.6106, KL-Loss   59.8434, KL-Weight  0.005\n",
            "TRAIN Epoch 01/10, Mean ELBO  339.5244\n",
            "Model saved at bin/2023-Apr-10-21:05:19/E1.pytorch\n",
            "VALID Batch 0000/47, Loss  319.7844, NLL-Loss  319.4533, KL-Loss   66.9961, KL-Weight  0.005\n",
            "VALID Batch 0047/47, Loss  309.9692, NLL-Loss  309.6700, KL-Loss   60.5371, KL-Weight  0.005\n",
            "VALID Epoch 01/10, Mean ELBO  317.4946\n",
            "TRAIN Batch 0000/188, Loss  302.7064, NLL-Loss  302.3804, KL-Loss   65.9481, KL-Weight  0.005\n",
            "TRAIN Batch 0050/188, Loss  307.2760, NLL-Loss  306.8940, KL-Loss   68.2631, KL-Weight  0.006\n",
            "TRAIN Batch 0100/188, Loss  309.5367, NLL-Loss  309.0935, KL-Loss   69.9469, KL-Weight  0.006\n",
            "TRAIN Batch 0150/188, Loss  305.4737, NLL-Loss  304.9608, KL-Loss   71.4772, KL-Weight  0.007\n",
            "TRAIN Batch 0188/188, Loss  292.6722, NLL-Loss  292.1373, KL-Loss   67.8426, KL-Weight  0.008\n",
            "TRAIN Epoch 02/10, Mean ELBO  298.1048\n",
            "Model saved at bin/2023-Apr-10-21:05:19/E2.pytorch\n",
            "VALID Batch 0000/47, Loss  300.9153, NLL-Loss  300.3599, KL-Loss   70.2700, KL-Weight  0.008\n",
            "VALID Batch 0047/47, Loss  292.8078, NLL-Loss  292.3045, KL-Loss   63.6768, KL-Weight  0.008\n",
            "VALID Epoch 02/10, Mean ELBO  305.0494\n",
            "TRAIN Batch 0000/188, Loss  250.3850, NLL-Loss  249.8524, KL-Loss   67.3982, KL-Weight  0.008\n",
            "TRAIN Batch 0050/188, Loss  269.3619, NLL-Loss  268.6790, KL-Loss   76.3294, KL-Weight  0.009\n",
            "TRAIN Batch 0100/188, Loss  275.4288, NLL-Loss  274.7014, KL-Loss   71.8321, KL-Weight  0.010\n",
            "TRAIN Batch 0150/188, Loss  270.3044, NLL-Loss  269.4574, KL-Loss   73.9213, KL-Weight  0.011\n",
            "TRAIN Batch 0188/188, Loss  282.6222, NLL-Loss  281.7006, KL-Loss   73.2227, KL-Weight  0.013\n",
            "TRAIN Epoch 03/10, Mean ELBO  267.8793\n",
            "Model saved at bin/2023-Apr-10-21:05:19/E3.pytorch\n",
            "VALID Batch 0000/47, Loss  296.0688, NLL-Loss  295.1346, KL-Loss   74.0377, KL-Weight  0.013\n",
            "VALID Batch 0047/47, Loss  287.5599, NLL-Loss  286.6718, KL-Loss   70.3866, KL-Weight  0.013\n",
            "VALID Epoch 03/10, Mean ELBO  300.4268\n",
            "TRAIN Batch 0000/188, Loss  229.5550, NLL-Loss  228.6172, KL-Loss   74.3237, KL-Weight  0.013\n",
            "TRAIN Batch 0050/188, Loss  232.5998, NLL-Loss  231.4580, KL-Loss   80.0020, KL-Weight  0.014\n",
            "TRAIN Batch 0100/188, Loss  243.7305, NLL-Loss  242.6044, KL-Loss   69.7548, KL-Weight  0.016\n",
            "TRAIN Batch 0150/188, Loss  257.3986, NLL-Loss  256.1619, KL-Loss   67.7511, KL-Weight  0.018\n",
            "TRAIN Batch 0188/188, Loss  281.7813, NLL-Loss  280.4552, KL-Loss   66.1843, KL-Weight  0.020\n",
            "TRAIN Epoch 04/10, Mean ELBO  246.1932\n",
            "Model saved at bin/2023-Apr-10-21:05:19/E4.pytorch\n",
            "VALID Batch 0000/47, Loss  291.8517, NLL-Loss  290.5350, KL-Loss   65.5571, KL-Weight  0.020\n",
            "VALID Batch 0047/47, Loss  283.8130, NLL-Loss  282.5225, KL-Loss   64.2511, KL-Weight  0.020\n",
            "VALID Epoch 04/10, Mean ELBO  299.6837\n",
            "TRAIN Batch 0000/188, Loss  227.0170, NLL-Loss  225.7080, KL-Loss   65.1735, KL-Weight  0.020\n",
            "TRAIN Batch 0050/188, Loss  223.2534, NLL-Loss  221.6082, KL-Loss   72.4828, KL-Weight  0.023\n",
            "TRAIN Batch 0100/188, Loss  228.5905, NLL-Loss  226.8500, KL-Loss   67.8710, KL-Weight  0.026\n",
            "TRAIN Batch 0150/188, Loss  239.9531, NLL-Loss  238.1405, KL-Loss   62.5941, KL-Weight  0.029\n",
            "TRAIN Batch 0188/188, Loss  243.1713, NLL-Loss  241.3762, KL-Loss   56.5310, KL-Weight  0.032\n",
            "TRAIN Epoch 05/10, Mean ELBO  230.5846\n",
            "Model saved at bin/2023-Apr-10-21:05:19/E5.pytorch\n",
            "VALID Batch 0000/47, Loss  292.0467, NLL-Loss  290.1668, KL-Loss   59.0604, KL-Weight  0.032\n",
            "VALID Batch 0047/47, Loss  281.8046, NLL-Loss  279.9397, KL-Loss   58.5894, KL-Weight  0.032\n",
            "VALID Epoch 05/10, Mean ELBO  300.5911\n",
            "TRAIN Batch 0000/188, Loss  213.5027, NLL-Loss  211.5936, KL-Loss   59.9784, KL-Weight  0.032\n",
            "TRAIN Batch 0050/188, Loss  233.2200, NLL-Loss  230.9818, KL-Loss   62.3174, KL-Weight  0.036\n",
            "TRAIN Batch 0100/188, Loss  210.6608, NLL-Loss  208.2971, KL-Loss   58.3578, KL-Weight  0.041\n",
            "TRAIN Batch 0150/188, Loss  229.7547, NLL-Loss  227.1590, KL-Loss   56.8614, KL-Weight  0.046\n",
            "TRAIN Batch 0188/188, Loss  226.0109, NLL-Loss  223.4488, KL-Loss   51.2680, KL-Weight  0.050\n",
            "TRAIN Epoch 06/10, Mean ELBO  219.6516\n",
            "Model saved at bin/2023-Apr-10-21:05:19/E6.pytorch\n",
            "VALID Batch 0000/47, Loss  293.5099, NLL-Loss  290.8148, KL-Loss   53.8042, KL-Weight  0.050\n",
            "VALID Batch 0047/47, Loss  281.6827, NLL-Loss  279.1429, KL-Loss   50.7020, KL-Weight  0.050\n",
            "VALID Epoch 06/10, Mean ELBO  301.9190\n",
            "TRAIN Batch 0000/188, Loss  213.1609, NLL-Loss  210.5322, KL-Loss   52.4779, KL-Weight  0.050\n",
            "TRAIN Batch 0050/188, Loss  204.8677, NLL-Loss  201.7050, KL-Loss   56.0902, KL-Weight  0.056\n",
            "TRAIN Batch 0100/188, Loss  206.7493, NLL-Loss  203.6017, KL-Loss   49.6326, KL-Weight  0.063\n",
            "TRAIN Batch 0150/188, Loss  212.9330, NLL-Loss  209.3469, KL-Loss   50.3248, KL-Weight  0.071\n",
            "TRAIN Batch 0188/188, Loss  219.2975, NLL-Loss  215.6910, KL-Loss   46.3512, KL-Weight  0.078\n",
            "TRAIN Epoch 07/10, Mean ELBO  211.9345\n",
            "Model saved at bin/2023-Apr-10-21:05:19/E7.pytorch\n",
            "VALID Batch 0000/47, Loss  292.6351, NLL-Loss  289.0475, KL-Loss   46.0017, KL-Weight  0.078\n",
            "VALID Batch 0047/47, Loss  277.7396, NLL-Loss  274.3332, KL-Loss   43.6792, KL-Weight  0.078\n",
            "VALID Epoch 07/10, Mean ELBO  302.9025\n",
            "TRAIN Batch 0000/188, Loss  194.9814, NLL-Loss  191.3292, KL-Loss   46.8302, KL-Weight  0.078\n",
            "TRAIN Batch 0050/188, Loss  206.6346, NLL-Loss  202.5649, KL-Loss   46.5307, KL-Weight  0.087\n",
            "TRAIN Batch 0100/188, Loss  189.5219, NLL-Loss  185.0949, KL-Loss   45.1884, KL-Weight  0.098\n",
            "TRAIN Batch 0150/188, Loss  203.9901, NLL-Loss  199.4652, KL-Loss   41.2918, KL-Weight  0.110\n",
            "TRAIN Batch 0188/188, Loss  229.3266, NLL-Loss  224.6084, KL-Loss   39.5816, KL-Weight  0.119\n",
            "TRAIN Epoch 08/10, Mean ELBO  206.3090\n",
            "Model saved at bin/2023-Apr-10-21:05:19/E8.pytorch\n",
            "VALID Batch 0000/47, Loss  295.9222, NLL-Loss  291.3456, KL-Loss   38.3086, KL-Weight  0.119\n",
            "VALID Batch 0047/47, Loss  280.7059, NLL-Loss  276.1299, KL-Loss   38.3039, KL-Weight  0.119\n",
            "VALID Epoch 08/10, Mean ELBO  304.8145\n",
            "TRAIN Batch 0000/188, Loss  187.8380, NLL-Loss  183.3875, KL-Loss   37.2536, KL-Weight  0.119\n",
            "TRAIN Batch 0050/188, Loss  197.6124, NLL-Loss  192.5833, KL-Loss   37.7409, KL-Weight  0.133\n",
            "TRAIN Batch 0100/188, Loss  210.6487, NLL-Loss  205.6142, KL-Loss   33.9336, KL-Weight  0.148\n",
            "TRAIN Batch 0150/188, Loss  213.7634, NLL-Loss  207.9794, KL-Loss   35.0843, KL-Weight  0.165\n",
            "TRAIN Batch 0188/188, Loss  206.6134, NLL-Loss  200.4508, KL-Loss   34.5516, KL-Weight  0.178\n",
            "TRAIN Epoch 09/10, Mean ELBO  202.3651\n",
            "Model saved at bin/2023-Apr-10-21:05:19/E9.pytorch\n",
            "VALID Batch 0000/47, Loss  296.4507, NLL-Loss  290.3806, KL-Loss   33.9629, KL-Weight  0.179\n",
            "VALID Batch 0047/47, Loss  278.2411, NLL-Loss  272.5081, KL-Loss   32.0772, KL-Weight  0.179\n",
            "VALID Epoch 09/10, Mean ELBO  307.4561\n"
          ]
        }
      ],
      "source": [
        "!python train.py -lr 0.004 -bs 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7oea-5KA1yx"
      },
      "source": [
        "Le Loss reste très grand même s'il diminu tout au long.\n",
        "En travvaillant encore plus sur les hyperparamètres, on peut eventuellement augmenter les performance de notre modèle mais compte tenu des limites d'unités de calculs notre ordinateur et le temps d'entrainement, nous sommes limités à jusque quelques variations des hyperparamètres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXq2TsggCTlr"
      },
      "source": [
        "### Generation de séquence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaImsFxJCklm"
      },
      "source": [
        "Nous utilisons notre modèles pour générer 10 séquances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsQW5dhXCCbn",
        "outputId": "a8750658-f0ab-400a-c90a-123a389c4589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from /content/drive/MyDrive/Sentence-VAE/bin/2023-Apr-10-21:05:19/E9.pytorch\n",
            "----------SAMPLES----------\n",
            "from kanebuast 7buedu hot young star subject re nazi eugenic theories circulated cpr organization university illinois urbana lines 19 in article c4tz28cpppanixcom roypanixcom roy radow writes in article <unk> walshoptilinkcom mark walsh writes in article c5og5h4deexnetcouk sys 1exnetcouk xavier gallagher writes in article 15312optilinkcom crameroptilinkcom clayton cramer writes from santa\n",
            "from tomgiftuxacsouiucedu tom haapanen subject re planets still images orbit by ether twist organization university illinois urbana lines 12 we recently admitted fairly low dollarpound area election inept leadership demand evening 6 month ago because course would like know way bypass develop cryptosystems specificially designed protect government financed purpose firstyear\n",
            "from <unk> terran boylan subject re rumours 3do organization basser dept computer science department rosehulman lines 17 in article <unk> <unk> <unk> nittmocamelotbradleyedu christopher mussack writes does anyone know possible thanks dick estelle <eos>\n",
            "from stephcsuiucedu dale adams subject re my gun like american express card inreplyto <unk> message tue 20 apr 93 <unk> gmt organization university illinois urbana lines 34 in article <unk> <unk> <unk> texdudecs 1bradleyedu philip russell writes in article <unk> <unk> <unk> texdudecs 1bradleyedu philip russell writes in article <unk>\n",
            "from lvccbnewscbattcom larry cipriani subject re gun backcountry organization california institute technology pasadena lines 13 distribution world nntppostinghost alumnicaltechedu in article <unk> fssetbachlercnasagov scott townsend writes i wa wondering anyone knew erickson keith emmen i wa wondering anyone knew erickson keith emmen i would like know exactly i would like\n",
            "from infanteacpubdukeedu andrew infante subject re the 1964 phillies deja vu organization duke university durham nc lines 29 in article <unk> davewoodbrunocscoloradoedu david rex wood writes shaky inning writes shaky inning giving one thing tend customize one thing tend filed hide term term lighting feeling causing counteracting lean tend fall\n",
            "from <unk> kenneth colby subject re how detect security articleid <unk> <unk> distribution na organization boeing computer services lines 12 nntppostinghost mbongoucsdedu in article <unk> <unk> <unk> garret <unk> writes huayong yang yangtitanucsumassedu april 24th approaching april 13 1993 milwaukee wi usa today quickdraw gx comprising hardware terminal unattended tttttt\n",
            "from <unk> philip j <unk> subject re canon bj200 bubblejet hp deskjet 500 organization dsg stanford university ca 94305 usa lines 12 i agree thouroughly i forgot addressbut <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "from <unk> jonathan goldstein subject re what ussr reached moon landingtemporary organization california polytechnic state university san luis obispo lines 13 in article <unk> frankoccocaltechedu frank filipanits writes in article 120399netnewsupennedu sepinwalmailsasupennedu alan sepinwall writes 1spike owen daily bread wa also sceptical critical autonomous incredible result clinical trial dispite guy\n",
            "from weverettjarthurclaremontedu william k yoder subject re how disk copy protected organization the ohio state university lines 17 in article shenoy jgreentrumpetcalpolyedu james thomas green pixel cruncher writes in article <unk> prbaccessdigexcom pat writes in article 1993apr262004061vax1mankatomsusedu belgarathvax 1mankatomsusedu writes energetic wa suggesting detail encrytion large amount penalty wa called\n",
            "-------INTERPOLATION-------\n",
            "from mathew mathewmantiscouk subject altatheism faq constructing logical organization mantis consultants cambridge uk xnewsreader tin version 11 pl8 lines 10 on thu 22 apr 1993 0454 03 gmt from 9150618 gavin fairlamb wrote hello everybody im looking information regarding new kuiper belt bra 90k sensor <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "from mathew mathewmantiscouk subject re need info circumcision medical con pro organization mantis consultants cambridge uk xnewsreader tin version 11 pl8 lines 10 on thu 22 apr 1993 2042 07 gmt from 9150618 gavin fairlamb subject re after 2000 year say christian parent mason organization university louisville lines 17 well massaging\n",
            "from mathew mathewmantiscouk subject re need info circumcision medical con pro organization mantis consultants cambridge uk xnewsreader rusnews v102 lines 10 on thu 22 apr 1993 0454 03 gmt from 9150618 gavin fairlamb subject re my question regarding overlaying report please note followup distribution usa lines 19 in article <unk> 4u28037uicvmuicedu\n",
            "from mathew mathewmantiscouk subject re need info circumcision medical con pro organization mantis consultants cambridge uk xnewsreader rusnews v102 lines 10 on thu 22 apr 1993 2042 04 gmt on thu 22 apr 1993 0454 03 gmt from 9150618 gavin fairlamb subject re my question regarding overlaying report please note followup i\n",
            "from denningguvaxaccgeorgetownedu subject re need info circumcision medical con pro con organization the university texas austin lines 17 distribution world replyto watsonmathsuwaeduau nntppostinghost louieccutexasedu comments summary which brings back key registering body incident i dont care enlighten access news group individuals the following survey directed towards biterrors organization university louisville\n",
            "from denningguvaxaccgeorgetownedu subject re need info circumcision medical con pro con organization the university texas austin lines 17 distribution world replyto watsonmathsuwaeduau nntppostinghost louieccutexasedu comments summary the opinion expressed user necessarily convex computer science fiction novelist i would like know opinion expressed user necessarily convex computer science private site would\n",
            "from <unk> subject re what zero db organization the university texas austin lines 17 distribution world nntppostinghost trampccutexasedu newssoftware vaxvms vnews 141 in article <unk> bradclarinetcom brad templeton writes the cryptographic algorythm must kept 24 hour day ago i wa wondering anyone knew supposedly wa wondering wa supposed obtain relevant\n",
            "from <unk> subject re what zero db organization express access online communications greenbelt md usa lines 17 nntppostinghost accessdigexnet in article <unk> shareniscnvxlmsclockheedcom sharen a rund writes i dont think i trust nsa supposed develop cryptosystems i dont think i dont care think clinton wa wondering anyone knew supposed dedicated\n",
            "from c445585mizzou1missouriedu john kelsey subject re how disk copy protected organization the ohio state university lines 23 distribution world nntppostinghost unlinfounledu summary the cryptographic algorythm ha escaped in article <unk> 4u28037uicvmuicedu jason kratz u28037uicvmuicedu writes i cant find information algorithm classified far far fullyautomatic algorithm classified far far behind bezel\n",
            "from c445585mizzou1missouriedu john kelsey subject re screw people crypto hardcore hacker spook organization the aerospace corporation el segundo ca lines 30 distribution world nntppostinghost carinaunmedu in article <unk> bradclarinetcom brad templeton writes i wa wondering anyone shed light clean 1000km i would like obtain relevant macweek 22293 i would like\n"
          ]
        }
      ],
      "source": [
        "!python inference.py -c '/content/drive/MyDrive/Sentence-VAE/bin/2023-Apr-10-21:05:19/E9.pytorch' -n 10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les textes générés sont incompréhensibles ce qui peut ce justifier par un manque d'ajustement des hyperpamarètres du modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
